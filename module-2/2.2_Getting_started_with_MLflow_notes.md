Configure mlflow with sqlite backend before launching ui so that data will be saved sqlite database.  
```bash
$ mlflow ui --backend-store-uri sqlite:///mlflow.db  
```

This tells mlfow that we want to store all artifacts and metadata in sqlite (one of the backends available).  
This is important otherwise mlflow cannot access history.

Create models folder to save models if not created already.  
 
In notebook:  

```python
import mlflow  

# Set tracking uri
mlflow.set_tracking_uri('sqlite:///mlflow.db')

# Create experiment if does not exists
mlflow.create_experiment('expt_name')
```

mlflow checks if experiment exists, creates if not, and then assigns all the runs to that experiment.
If already exists, then append runs to that experiment.  

This creates an experiment object  
```
<Experiment: artifact_location='/home/noobdv18/projects/mlops-zoomcamp/module-2/mlruns/1', creation_time=1746858035246, experiment_id='1', last_update_time=1746858035246, lifecycle_stage='active', name='nyc-taxi-experiment', tags={}>  
```

It has created mlruns folder to save aritfiacts.  

Simple way to add tracking is to define a new run:   

```python
#Define new run

with mlflow.start_run():

    # Log all information here

    # Name of developer
    mlflow.set_tag("developer", "omkar")

    # Log parameters including hyperps as well as any other relavant thing that affects model performance.
    mlflow.log_param("train_data_path", "module-2/data/green_tripdata_2021-01.parquet")
    mlflow.log_param("val_data_path", "module-2/data/green_tripdata_2021-02.parquet")

    # Log parameter alpha
    alpha = 0.01
    mlflow.log_param("alpha", alpha)
    # Use Lasso regression: Linear regression with regularization

    model = Ridge(alpha=alpha)
    model.fit(X_train, y_train)

    y_pred = model.predict(X_train)

    rmse = math.sqrt(mean_squared_error(y_train, y_pred))

    # Log (track) RMSE
    mlflow.log_metric("RMSE", rmse)

```
Everything inside with statement is associated with current run.  
parameters including hyperps as well as any other relavant information that affects model performance.  
eg. train data path    

Log params using `mlflow.log_param("name_of_param", value)`.      
Log metrics using `mlflow.log_metric("'name_of_param", value)`.    

After new runs ends executing, mlflow ui will have notificatios indicatiog run runs. Or     
in latest version it automatically adds run in ui.    

You can see metrics, and plot of metrics if run for multiple epochs.  