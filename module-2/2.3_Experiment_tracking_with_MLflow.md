- Add hyper parameter tuning into noteobok.  
- Explore results and select best model base on results.


- Train model using xgboost.
- Tune with hyperopt
- Log all information into mlflow

hyperopt : Bayesian hyperparameter searchin g library
- Your model is function that takes parameters and outputs some value as output.
- fmin: used to minimize output of model function.
- tpe: algorithm that controls the logic
- hp : module contains methods that defines search space (range for each hyperp)
- STATUS_OK: signal that we will send at end of each run to tell hyperopt that objectve funciton has run successfully.
- Trials: object that keeps track of information from each run.

```python
search_space = {
    'max_depth': scope.int(hp.quniform('max_depth', 4, 100, 1)),
    'learning_rate': hp.loguniform('learning_rate', -3, 0),  # [exp(-3), exp(0)] = [~0.05, 1]
    'reg_alpha': hp.loguniform('reg_alpha', -5, -1),
    'reg_lambda': hp.loguniform('reg_lambda', -6, -1),
    'min_child_weight': hp.loguniform('min_child_weight', -1, 3),
    'objective': 'reg:squared_error',
    'seed': 40
}
```
Now pass all information to fmin() which will try to optimize objective() function by minimizing the output. 